import streamlit as st
import json
import pandas as pd
from scrape_utils import (
    scrape_website_brightdata,
    scrape_website_nobright,
    scrape_website_combined,
    scrape_website_nobright_only,
    extract_body_content,
    clean_body_content,
    clean_body_content_with_links_and_images,
    split_dom_content,
    analyze_content_for_missing_data,
)
from parse import parse_with_ollama

# Streamlit UI
st.title("AI Web Scraper")
url = st.text_input("Enter Website URL")

# Ch·ªçn ph∆∞∆°ng th·ª©c scraping
scrape_method = st.selectbox(
    "Ch·ªçn ph∆∞∆°ng th·ª©c scraping:",
    ("T·ª± ƒë·ªông (Combined)", "Ch·ªâ Chrome (No BrightData)", "BrightData")
)

# T√πy ch·ªçn crawl links v√† images
include_links_images = st.checkbox(
    "Crawl links v√† ƒë·ªãa ch·ªâ ·∫£nh", 
    value=False,
    help="Khi b·∫≠t, s·∫Ω thu th·∫≠p th√¥ng tin v·ªÅ links v√† URLs c·ªßa h√¨nh ·∫£nh"
)

# T√πy ch·ªçn n√¢ng cao
with st.expander("‚öôÔ∏è C√†i ƒë·∫∑t n√¢ng cao"):
    col1, col2 = st.columns(2)
    with col1:
        chunk_size = st.selectbox(
            "K√≠ch th∆∞·ªõc chunk x·ª≠ l√Ω:",
            [4000, 8000, 12000, 16000, 20000],
            index=1,  # Default 6000
            help="Chunk nh·ªè h∆°n = x·ª≠ l√Ω ch√≠nh x√°c h∆°n nh∆∞ng ch·∫≠m h∆°n"
        )
    with col2:
        max_chunks = st.selectbox(
            "S·ªë chunks t·ªëi ƒëa:",
            [30, 50, 70, 100],
            index=1,  # Default 50
            help="TƒÉng ƒë·ªÉ x·ª≠ l√Ω nhi·ªÅu d·ªØ li·ªáu h∆°n"
        )

# Step 1: Scrape the Website
if st.button("Scrape Website"):
    if url:
        st.write("ƒêang scraping website...")

        try:
            # Ch·ªçn ph∆∞∆°ng th·ª©c scraping d·ª±a tr√™n l·ª±a ch·ªçn c·ªßa ng∆∞·ªùi d√πng
            if scrape_method == "Ch·ªâ Chrome (No BrightData)":
                dom_content = scrape_website_nobright_only(url)
            elif scrape_method == "BrightData":
                dom_content = scrape_website_brightdata(url)
            else:  # T·ª± ƒë·ªông (Combined)
                dom_content = scrape_website_combined(url)
                
            body_content = extract_body_content(dom_content)
            
            # S·ª≠ d·ª•ng h√†m cleaning ph√π h·ª£p d·ª±a tr√™n l·ª±a ch·ªçn c·ªßa ng∆∞·ªùi d√πng
            if include_links_images:
                cleaned_content = clean_body_content_with_links_and_images(body_content, base_url=url)
            else:
                cleaned_content = clean_body_content(body_content)

            # Store the DOM content in Streamlit session state
            st.session_state.dom_content = cleaned_content

            # Display the DOM content in an expandable text box
            with st.expander("Xem n·ªôi dung DOM"):
                st.text_area("N·ªôi dung DOM", cleaned_content, height=300)
                
            # Hi·ªÉn th·ªã th√¥ng b√°o v·ªÅ mode crawling
            if include_links_images:
                st.success("Scraping th√†nh c√¥ng! ‚úÖ ƒê√£ bao g·ªìm links v√† ƒë·ªãa ch·ªâ ·∫£nh")
            else:
                st.success("Scraping th√†nh c√¥ng! ‚ÑπÔ∏è Ch·ªâ text content (kh√¥ng bao g·ªìm links/·∫£nh)")
            
        except Exception as e:
            st.error(f"L·ªói khi scraping: {str(e)}")
            st.info("H√£y th·ª≠ s·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c 'Ch·ªâ Chrome (No BrightData)' n·∫øu b·∫°n kh√¥ng c√≥ c·∫•u h√¨nh BrightData.")


# Step 2: Ask Questions About the DOM Content
if "dom_content" in st.session_state:
    st.subheader("B∆∞·ªõc 2: Ph√¢n t√≠ch n·ªôi dung")
    
    # Th√™m h∆∞·ªõng d·∫´n
    with st.expander("üí° H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng"):
        st.write("""
        **C√°ch m√¥ t·∫£ th√¥ng tin c·∫ßn tr√≠ch xu·∫•t:**
        - Li·ªát k√™ c√°c th√¥ng tin c·∫ßn l·∫•y, c√°ch nhau b·∫±ng d·∫•u ph·∫©y
        - V√≠ d·ª•: "H·ªç v√† t√™n, NƒÉm sinh, Qu√™ qu√°n, Tr√¨nh ƒë·ªô chuy√™n m√¥n, Ch·ª©c v·ª•, ƒêo√†n ƒêBQH, ƒê·∫°t % s·ªë phi·∫øu"
        
        **File Excel s·∫Ω c√≥ c√°c c·ªôt t∆∞∆°ng ·ª©ng:**
        - H·ªç v√† t√™n ‚Üí HoVaTen
        - NƒÉm sinh ‚Üí NamSinh  
        - Qu√™ qu√°n ‚Üí QueQuan
        - Tr√¨nh ƒë·ªô chuy√™n m√¥n ‚Üí TrinhDoChuyenMon
        - Ch·ª©c v·ª• ‚Üí ChucVu
        - ƒêo√†n ƒêBQH ‚Üí DoanDBQH
        - ƒê·∫°t % s·ªë phi·∫øu ‚Üí SoPhieu
        
        **Khi b·∫≠t "Crawl links v√† ƒë·ªãa ch·ªâ ·∫£nh":**
        - Links s·∫Ω hi·ªÉn th·ªã d·∫°ng: "Text link [LINK: url]"
        - H√¨nh ·∫£nh s·∫Ω hi·ªÉn th·ªã d·∫°ng: "[IMAGE: Alt: m√¥ t·∫£ | URL: ƒë·ªãa ch·ªâ]"
        - Video/Audio s·∫Ω hi·ªÉn th·ªã d·∫°ng: "[VIDEO: url]" ho·∫∑c "[AUDIO: url]"
        - B·∫°n c√≥ th·ªÉ y√™u c·∫ßu AI tr√≠ch xu·∫•t: "URLs c·ªßa h√¨nh ·∫£nh, Links trang web, ƒê·ªãa ch·ªâ video"
        """)
    
    # Thay ƒë·ªïi placeholder d·ª±a tr√™n vi·ªác c√≥ b·∫≠t crawl links/images hay kh√¥ng
    if include_links_images:
        placeholder_text = "V√≠ d·ª•: H·ªç v√† t√™n, NƒÉm sinh, URLs h√¨nh ·∫£nh, Links trang web, ƒê·ªãa ch·ªâ video"
        help_text = "Nh·∫≠p c√°c th√¥ng tin b·∫°n mu·ªën tr√≠ch xu·∫•t, bao g·ªìm links v√† URLs media, c√°ch nhau b·∫±ng d·∫•u ph·∫©y"
    else:
        placeholder_text = "V√≠ d·ª•: H·ªç v√† t√™n, NƒÉm sinh, Qu√™ qu√°n, Tr√¨nh ƒë·ªô chuy√™n m√¥n, Ch·ª©c v·ª•, ƒêo√†n ƒêBQH, ƒê·∫°t % s·ªë phi·∫øu"
        help_text = "Nh·∫≠p c√°c th√¥ng tin b·∫°n mu·ªën tr√≠ch xu·∫•t, c√°ch nhau b·∫±ng d·∫•u ph·∫©y"
    
    parse_description = st.text_area(
        "M√¥ t·∫£ nh·ªØng g√¨ b·∫°n mu·ªën ph√¢n t√≠ch t·ª´ n·ªôi dung:",
        placeholder=placeholder_text,
        help=help_text
    )

    if st.button("Ph√¢n t√≠ch n·ªôi dung"):
        if parse_description:
            st.write("ƒêang ph√¢n t√≠ch n·ªôi dung...")

            try:
                # Parse the content with Ollama
                dom_chunks = split_dom_content(
                    st.session_state.dom_content, 
                    max_length=chunk_size, 
                    max_batches=max_chunks
                )
                parsed_result = parse_with_ollama(dom_chunks, parse_description)
                
                # L∆∞u k·∫øt qu·∫£ v√†o session state
                st.session_state.parsed_data = parsed_result
                
                # Hi·ªÉn th·ªã th·ªëng k√™ x·ª≠ l√Ω
                if 'stats' in parsed_result:
                    stats = parsed_result['stats']
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("T·ªïng chunks", stats['total_chunks'])
                    with col2:
                        st.metric("Chunks th√†nh c√¥ng", stats['successful_chunks'])
                    with col3:
                        st.metric("T·ªïng records", stats['total_records'])
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng b·∫£ng n·∫øu c√≥ d·ªØ li·ªáu structured
                if parsed_result['structured_data']:
                    st.success(f"üéâ ƒê√£ ph√¢n t√≠ch ƒë∆∞·ª£c {len(parsed_result['structured_data'])} m·ª•c d·ªØ li·ªáu c√≥ c·∫•u tr√∫c!")
                    
                    # T·∫°o DataFrame ƒë·ªÉ hi·ªÉn th·ªã
                    df = pd.DataFrame(parsed_result['structured_data'])
                    st.subheader("D·ªØ li·ªáu ƒë√£ ph√¢n t√≠ch:")
                    st.dataframe(df, use_container_width=True)
                    
                    # Hi·ªÉn th·ªã th√¥ng tin v·ªÅ c√°c c·ªôt
                    st.info(f"C√°c c·ªôt d·ªØ li·ªáu: {', '.join(df.columns.tolist())}")
                    
                    # Hi·ªÉn th·ªã c·∫£nh b√°o n·∫øu c√≥ v·∫ª nh∆∞ thi·∫øu d·ªØ li·ªáu
                    if 'stats' in parsed_result and parsed_result['stats']['total_records'] < 50:
                        st.warning("‚ö†Ô∏è S·ªë l∆∞·ª£ng records c√≥ v·∫ª th·∫•p. ƒêang ph√¢n t√≠ch nguy√™n nh√¢n...")
                        
                        # Ph√¢n t√≠ch content ƒë·ªÉ t√¨m nguy√™n nh√¢n
                        analysis = analyze_content_for_missing_data(st.session_state.dom_content)
                        
                        if analysis['potential_issues']:
                            st.write("**C√°c v·∫•n ƒë·ªÅ ph√°t hi·ªán:**")
                            for issue in analysis['potential_issues']:
                                st.write(f"- {issue}")
                        
                        if analysis['suggestions']:
                            st.write("**G·ª£i √Ω kh·∫Øc ph·ª•c:**")
                            for suggestion in analysis['suggestions']:
                                st.write(f"- {suggestion}")
                        
                        st.write("**C√°c b∆∞·ªõc ki·ªÉm tra kh√°c:**")
                        st.write("- Ki·ªÉm tra m√¥ t·∫£ ph√¢n t√≠ch c√≥ ch√≠nh x√°c kh√¥ng")
                        st.write("- Th·ª≠ scrape l·∫°i website v·ªõi th·ªùi gian ch·ªù l√¢u h∆°n")
                        st.write("- Th·ª≠ tƒÉng k√≠ch th∆∞·ªõc chunk ho·∫∑c s·ªë chunks t·ªëi ƒëa")
                else:
                    # Hi·ªÉn th·ªã text n·∫øu kh√¥ng c√≥ structured data
                    st.write(parsed_result['combined_text'])
                
            except Exception as e:
                st.error(f"L·ªói khi ph√¢n t√≠ch: {str(e)}")

# Step 3: Download buttons
if "parsed_data" in st.session_state:
    st.subheader("T·∫£i xu·ªëng d·ªØ li·ªáu ƒë√£ ph√¢n t√≠ch")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # N√∫t t·∫£i JSON
        if st.session_state.parsed_data['structured_data']:
            json_data = json.dumps(st.session_state.parsed_data['structured_data'], 
                                 ensure_ascii=False, indent=2)
            st.download_button(
                label="üìÅ T·∫£i d·ªØ li·ªáu JSON",
                data=json_data,
                file_name=f"parsed_data_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
        else:
            # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu structured, t·∫£i text results
            json_data = json.dumps(st.session_state.parsed_data['text_results'], 
                                 ensure_ascii=False, indent=2)
            st.download_button(
                label="üìÅ T·∫£i d·ªØ li·ªáu JSON (Text)",
                data=json_data,
                file_name=f"parsed_text_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
    
    with col2:
        # N√∫t t·∫£i Excel
        if st.session_state.parsed_data['structured_data']:
            try:
                import pandas as pd
                import io
                
                df = pd.DataFrame(st.session_state.parsed_data['structured_data'])
                
                # T·∫°o buffer ƒë·ªÉ l∆∞u file Excel
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False, sheet_name='Parsed Data')
                    
                    # L·∫•y worksheet ƒë·ªÉ format
                    worksheet = writer.sheets['Parsed Data']
                    
                    # Auto-adjust column width
                    for column in worksheet.columns:
                        max_length = 0
                        column_letter = column[0].column_letter
                        for cell in column:
                            try:
                                if len(str(cell.value)) > max_length:
                                    max_length = len(str(cell.value))
                            except:
                                pass
                        adjusted_width = min(max_length + 2, 50)  # Gi·ªõi h·∫°n ƒë·ªô r·ªông t·ªëi ƒëa
                        worksheet.column_dimensions[column_letter].width = adjusted_width
                
                st.download_button(
                    label="üìä T·∫£i d·ªØ li·ªáu Excel",
                    data=buffer.getvalue(),
                    file_name=f"parsed_data_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
                
                # Hi·ªÉn th·ªã preview c·ªßa Excel
                st.write("**Preview d·ªØ li·ªáu Excel:**")
                st.dataframe(df.head(), use_container_width=True)
                
            except ImportError:
                st.error("C·∫ßn c√†i ƒë·∫∑t pandas v√† openpyxl ƒë·ªÉ xu·∫•t Excel. Ch·∫°y: pip install pandas openpyxl")
            except Exception as e:
                st.error(f"L·ªói khi t·∫°o file Excel: {str(e)}")
        else:
            # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu structured, t·∫°o Excel t·ª´ text
            try:
                import pandas as pd
                import io
                
                # T·∫°o DataFrame t·ª´ text results
                df = pd.DataFrame({
                    'Batch': [f"Batch {i+1}" for i in range(len(st.session_state.parsed_data['text_results']))],
                    'Content': st.session_state.parsed_data['text_results']
                })
                
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False, sheet_name='Parsed Text')
                    
                    # Format worksheet
                    worksheet = writer.sheets['Parsed Text']
                    worksheet.column_dimensions['A'].width = 15
                    worksheet.column_dimensions['B'].width = 80
                
                st.download_button(
                    label="üìä T·∫£i d·ªØ li·ªáu Excel (Text)",
                    data=buffer.getvalue(),
                    file_name=f"parsed_text_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
            except ImportError:
                st.error("C·∫ßn c√†i ƒë·∫∑t pandas v√† openpyxl ƒë·ªÉ xu·∫•t Excel. Ch·∫°y: pip install pandas openpyxl")
            except Exception as e:
                st.error(f"L·ªói khi t·∫°o file Excel: {str(e)}")
