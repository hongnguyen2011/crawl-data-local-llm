"""
#     "4. **Direct Data Only:** Your output should contain only the data that is explicitly requested, with no other text.
#     "1. **Extract Information:** extract the information that directly matches the provided description: {parse_description}. "
# 2. "**No Extra Content:** Do not include any additional text, comments, or explanations in your response. 

"Please follow these instructions carefully: \n\n"
    "1. **Basic setting** you basic setting is to return most of the text content as long as it matches the provided description: {parse_description} "
    "2. We are specifically interested in the text that explain concepts, code explanations, the content of code blocks, and tables related to the topic. "
    "3. Dont return comments about the content or what you are about to do, if the information dont match the provided description, return all the content {dom_content} ."
"""


from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
import pandas as pd
import json
import re
import os
from dotenv import load_dotenv

load_dotenv()

template = (
    "You are tasked with extracting and structuring information from the following text content: {dom_content}. "
    "Based on the user's description: {parse_description}, extract the relevant data and return it as a JSON object. "
    
    "CRITICAL EXTRACTION REQUIREMENTS: "
    "- Extract ALL matching records from the content, not just a few examples "
    "- If there are 50 people mentioned, extract ALL 50 people "
    "- If there are 100 products listed, extract ALL 100 products "
    "- DO NOT limit yourself to only a few items - extract EVERYTHING that matches "
    
    "IMPORTANT FIELD NAMING RULES: "
    "- Use Vietnamese field names without spaces or special characters "
    "- Convert Vietnamese field names to camelCase format "
    "- Examples: 'H·ªç v√† t√™n' ‚Üí 'HoVaTen', 'NƒÉm sinh' ‚Üí 'NamSinh', 'Qu√™ qu√°n' ‚Üí 'QueQuan', "
    "'Tr√¨nh ƒë·ªô chuy√™n m√¥n' ‚Üí 'TrinhDoChuyenMon', 'Ch·ª©c v·ª•' ‚Üí 'ChucVu', 'ƒêo√†n ƒêBQH' ‚Üí 'DoanDBQH', "
    "'ƒê·∫°t % s·ªë phi·∫øu' ‚Üí 'SoPhieu', 'S·ªë phi·∫øu' ‚Üí 'SoPhieu', 'ƒê·ªãa ch·ªâ ·∫£nh' ‚Üí 'DiaChiAnh', "
    "'URLs h√¨nh ·∫£nh' ‚Üí 'URLsHinhAnh', 'Links' ‚Üí 'Links', 'Li√™n k·∫øt' ‚Üí 'LienKet' "
    
    "SPECIAL HANDLING FOR LINKS AND MEDIA: "
    "- Links appear as: 'text [LINK: url]' - extract both text and URL if requested "
    "- Images appear as: '[IMAGE: Alt: description | URL: image_url]' - extract URLs if requested "
    "- Videos appear as: '[VIDEO: video_url]' - extract URLs if requested "
    "- Audio appears as: '[AUDIO: audio_url]' - extract URLs if requested "
    
    "OUTPUT FORMAT: "
    "- If you find multiple items (like multiple people), return them as a list of JSON objects with the same field structure "
    "- Each JSON object should represent one complete record (e.g., one person's information) "
    "- Extract ALL records found in this content chunk, not just the first few "
    "- Only return valid JSON, no additional text or explanations "
    "- If no matching data found, return an empty list: []"
)

# L·∫•y t√™n model t·ª´ bi·∫øn m√¥i tr∆∞·ªùng, m·∫∑c ƒë·ªãnh l√† llama3:latest
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3:latest")
model = OllamaLLM(model=OLLAMA_MODEL)

def normalize_field_names(data):
    """
    Chu·∫©n h√≥a t√™n field trong d·ªØ li·ªáu JSON ƒë·ªÉ ph√π h·ª£p v·ªõi Excel
    """
    field_mapping = {
        # H·ªç v√† t√™n
        'h·ªç v√† t√™n': 'HoVaTen',
        'h·ªç t√™n': 'HoVaTen', 
        't√™n': 'HoVaTen',
        'name': 'HoVaTen',
        'fullname': 'HoVaTen',
        'ho_va_ten': 'HoVaTen',
        
        # NƒÉm sinh
        'nƒÉm sinh': 'NamSinh',
        'birth_year': 'NamSinh',
        'year_of_birth': 'NamSinh',
        'nam_sinh': 'NamSinh',
        
        # Qu√™ qu√°n
        'qu√™ qu√°n': 'QueQuan',
        'que_quan': 'QueQuan',
        'hometown': 'QueQuan',
        'origin': 'QueQuan',
        
        # Tr√¨nh ƒë·ªô chuy√™n m√¥n
        'tr√¨nh ƒë·ªô chuy√™n m√¥n': 'TrinhDoChuyenMon',
        'trinh_do_chuyen_mon': 'TrinhDoChuyenMon',
        'education': 'TrinhDoChuyenMon',
        'qualification': 'TrinhDoChuyenMon',
        
        # Ch·ª©c v·ª•
        'ch·ª©c v·ª•': 'ChucVu',
        'chuc_vu': 'ChucVu',
        'position': 'ChucVu',
        'title': 'ChucVu',
        
        # ƒêo√†n ƒêBQH
        'ƒëo√†n ƒëbqh': 'DoanDBQH',
        'doan_dbqh': 'DoanDBQH',
        'delegation': 'DoanDBQH',
        
        # S·ªë phi·∫øu
        'ƒë·∫°t % s·ªë phi·∫øu': 'SoPhieu',
        's·ªë phi·∫øu': 'SoPhieu',
        'so_phieu': 'SoPhieu',
        'votes': 'SoPhieu',
        'vote_percentage': 'SoPhieu',
        
        # Links v√† Images
        'links': 'Links',
        'link': 'Links',
        'url': 'URL',
        'urls': 'URLs',
        'image': 'HinhAnh',
        'images': 'HinhAnh',
        'h√¨nh ·∫£nh': 'HinhAnh',
        '·∫£nh': 'HinhAnh',
        'img': 'HinhAnh',
        'video': 'Video',
        'audio': 'Audio',
        'media': 'Media',
        'ƒë·ªãa ch·ªâ ·∫£nh': 'DiaChiAnh',
        'ƒë∆∞·ªùng d·∫´n': 'DuongDan',
        'li√™n k·∫øt': 'LienKet'
    }
    
    def normalize_item(item):
        if isinstance(item, dict):
            normalized = {}
            for key, value in item.items():
                # T√¨m key mapping ph√π h·ª£p
                normalized_key = key
                for original, mapped in field_mapping.items():
                    if original.lower() in key.lower():
                        normalized_key = mapped
                        break
                normalized[normalized_key] = value
            return normalized
        return item
    
    if isinstance(data, list):
        return [normalize_item(item) for item in data]
    elif isinstance(data, dict):
        return normalize_item(data)
    return data

def parse_with_ollama(dom_chunks, parse_description):
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | model

    parsed_results = []
    all_data = []
    successful_chunks = 0
    total_records_extracted = 0

    print(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(dom_chunks)} chunks...")
    
    for i, chunk in enumerate(dom_chunks, start=1):
        print(f"ƒêang x·ª≠ l√Ω chunk {i}/{len(dom_chunks)} (k√≠ch th∆∞·ªõc: {len(chunk)} k√Ω t·ª±)...")
        
        response = chain.invoke(
            {"dom_content": chunk, "parse_description": parse_description}
        )
        parsed_results.append(response)
        
        # Th·ª≠ parse JSON t·ª´ response
        try:
            # L√†m s·∫°ch response ƒë·ªÉ l·∫•y JSON
            cleaned_response = clean_json_response(response)
            json_data = json.loads(cleaned_response)
            
            # Chu·∫©n h√≥a t√™n field
            json_data = normalize_field_names(json_data)
            
            # ƒê·∫øm s·ªë records t·ª´ chunk n√†y
            chunk_records = 0
            
            # N·∫øu l√† list, extend v√†o all_data
            if isinstance(json_data, list):
                chunk_records = len(json_data)
                all_data.extend(json_data)
            # N·∫øu l√† dict, append v√†o all_data
            elif isinstance(json_data, dict) and json_data:
                chunk_records = 1
                all_data.append(json_data)
            
            if chunk_records > 0:
                successful_chunks += 1
                total_records_extracted += chunk_records
                print(f"‚úÖ Chunk {i}: Tr√≠ch xu·∫•t ƒë∆∞·ª£c {chunk_records} records")
            else:
                print(f"‚ö†Ô∏è Chunk {i}: Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p")
                
        except (json.JSONDecodeError, Exception) as e:
            print(f"‚ùå Chunk {i}: Kh√¥ng th·ªÉ parse JSON - {e}")
            # In ra m·ªôt ph·∫ßn response ƒë·ªÉ debug
            print(f"Response preview: {response[:200]}...")
            continue

    print(f"\nüìä K·∫øt qu·∫£ t·ªïng h·ª£p:")
    print(f"- T·ªïng chunks: {len(dom_chunks)}")
    print(f"- Chunks th√†nh c√¥ng: {successful_chunks}")
    print(f"- T·ªïng records tr√≠ch xu·∫•t: {total_records_extracted}")
    
    # L∆∞u d·ªØ li·ªáu v√†o session state ƒë·ªÉ c√≥ th·ªÉ t·∫£i xu·ªëng
    return {
        'text_results': parsed_results,
        'structured_data': all_data,
        'combined_text': "\n".join(parsed_results),
        'stats': {
            'total_chunks': len(dom_chunks),
            'successful_chunks': successful_chunks,
            'total_records': total_records_extracted
        }
    }

def clean_json_response(response):
    """
    L√†m s·∫°ch response ƒë·ªÉ l·∫•y JSON h·ª£p l·ªá
    """
    # T√¨m JSON trong response
    json_match = re.search(r'\{.*\}|\[.*\]', response, re.DOTALL)
    if json_match:
        return json_match.group()
    
    # N·∫øu kh√¥ng t√¨m th·∫•y JSON pattern, th·ª≠ l√†m s·∫°ch basic
    cleaned = response.strip()
    if cleaned.startswith('```json'):
        cleaned = cleaned[7:]
    if cleaned.endswith('```'):
        cleaned = cleaned[:-3]
    
    return cleaned.strip()
